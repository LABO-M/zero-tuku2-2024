{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ２章　自然言語と単語の分散表現\n",
    "\n",
    "### 自然言語処理とは？\n",
    "私達が普段使う日本語や英語などを**自然言語**といい、自然言語処理とはこれをコンピュータに理解させる技術・分野のことである。  \n",
    "\n",
    "言葉の意味の最小単位は「単語」です。この単語の意味をコンピュータに理解させる方法として３つ紹介します。  \n",
    "\n",
    "- シソーラスによる手法  \n",
    "\n",
    "- カウントベースの手法  \n",
    "\n",
    "- 推論ベースの手法(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#必要なライブラリのimport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シソーラスとは\n",
    "**シソーラス**は類語辞書です。  \n",
    "  \n",
    "例えば\"car\"の類語には\"autpmpbile\"や\"motorcar\"があり、シソーラスには同じグループとして定義されています。  \n",
    "  \n",
    "また、シソーラスには単語の縦の関係性も定義されている場合があり、\"car\"であればその上には\"motor vehicle\"のように抽象的な単語、下には\"SUV\"などの具体的な単語が保存されています。  \n",
    "  \n",
    "### シソーラスの問題点\n",
    "  \n",
    "このシソーラスを使えばコンピュータに単語の意味を教えられそうですが、もちろん問題点もあります。  \n",
    "  \n",
    "時代の変化に対応するのが難しい、人の作業コストが高い、単語の細かいニュアンスを表現できない。  \n",
    "  \n",
    "このように人手によって意味を定義しようとするとどうしてもうまくいきません。ですので次からは人手を使わない手法を紹介していこうと思います。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カウントベース\n",
    "それではここからはシソーラスの代替案としてカウントベースの手法を紹介します。まずはコーパス呼ばれる大量のテキストデータを用意します。（はじめは短い一文のみで構成されたものを扱います。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hello.' #テキストデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text を単語単位に分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you say goodbye and i say hello .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower() #小文字に変換\n",
    "text = text.replace('.', ' .') #ピリオドの前にスペースを入れる\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(' ') #スペースで区切る\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の操作によってコーパスを単語のリストとして利用できるようになりました。\n",
    "次は単語にIDを振って、IDのリストとして利用できるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {} #単語をIDに変換する辞書\n",
    "id_to_word = {} #IDを単語に変換する辞書\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id: # 未登録の単語の場合\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id #新しい単語を辞書に追加\n",
    "        id_to_word[new_id] = word #新しいIDを辞書に追加                                                                                                      \n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[1] #IDから単語を引く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id['hello'] #単語からIDを引く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のID付ができました！\n",
    "単語のリストを単語IDリストへ変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 1, 5, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [word_to_id[w] for w in words] #単語IDのリスト\n",
    "print(corpus)\n",
    "corpus = np.array(corpus) #numpyの配列に変換\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の操作（コーパスから単語IDへの変換）を前処理として関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#コーパスの前処理を関数化\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "            \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "#実験\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus) #単語IDのリスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の分散表現\n",
    "\n",
    "色の表現方法  \n",
    "  \n",
    "固有の名前  \n",
    "    \n",
    "それぞれの色につけられた名前。赤や青などのシンプルなものから複雑な色まで全てに固有のものが存在する。  \n",
    "  \n",
    "例：「コバルトブルー」、「マゼンタ」、「萌葱」、「山吹」など  \n",
    "  \n",
    "ベクトル表記  \n",
    "    \n",
    "ＲＧＢの３成分のベクトルとして表現する。どの成分がどれくらいあるか一目でわかる。  \n",
    "  \n",
    "例：萌葱→(R,G,B)=(0,108,79)  \n",
    "  \n",
    "単語もベクトル表現できないか？→**単語の分散表現**    \n",
    "  \n",
    "単語をベクトルで表現しようとするときに最初にぶつかるのは**何を数値としてベクトルにするか**です。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語処理の分野では「単語の意味は周囲の単語によって形成される」よいう大胆でシンプルな仮説がよく用いられ、今回もこれを駆使してベクトル表現をしていきます。  \n",
    "  \n",
    "カウントベースの手法では名前の通りある単語の周囲にどのような単語がどれだけ現れるかをカウントし、集計します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../pictures/pict1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語\"say\"に対して周囲に現れる単語の頻度を表で表しました。 \n",
    "   \n",
    "これは\"say\"という単語が[1,0,1,0,1,1,0]というベクトルで表現できるということを意味します。  \n",
    "  \n",
    "他の単語も同様に表し、まとめると下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../pictures/pict2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "#下準備\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#共起行列の作成(手動)\n",
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[0]) #単語IDが0のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[4]) #単語IDが4のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[word_to_id['goodbye']]) #goodbyeのベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#共起行列を作成する関数\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)#共起行列の初期化\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus): # np.ndenumerate()は多次元配列のインデックスと要素を返す\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx] # 左側の単語IDを取得\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx] # 右側の単語IDを取得\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパス（テキスト）から共起行列を作成してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0 0]\n",
      " [0 1 0 1 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0]\n",
      " [0 1 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#実験\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(corpus)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで共起行列を作ることができたので次はベクトル同士の類似度を見ていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベクトルの類似度\n",
    "コサイン類似度の定義式\n",
    "![alt text](../pictures/pict3.png)  \n",
    "直感的には「２つのベクトルがどれだけ同じ方向を向いているか」を表します。\n",
    "値域は[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#コサイン類似度を求める関数\n",
    "def cos_similarity(x, y, eps=1e-8): #epsは0割を防ぐための微小値\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps) #xの正規化\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps) #yの正規化\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067758832467\n"
     ]
    }
   ],
   "source": [
    "#実験\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]  #「you」の単語ベクトル\n",
    "c1 = C[word_to_id['i']]  #「i」の単語ベクトル\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「you」と「i」コサイン類似度は０．７０となった。\n",
    "この値は比較的高い値、類似性があるといえそう\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似単語のランキング表示\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    #クエリを取り出す\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query] #クエリの単語ID\n",
    "    query_vec = word_matrix[query_id] #クエリの単語ベクトル\n",
    "    \n",
    "    #コサイン類似度の算出\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size) \n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)# 全単語とのコサイン類似度を算出 \n",
    "\n",
    "    #コサイン類似度の結果から、その値を高い順に出力    \n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_similar()の引数\n",
    "| 引数名 | 説明 |\n",
    "|--------|------|\n",
    "| query  | クエリとして検索する単語 |\n",
    "| word_to_id | 単語をIDに変換する辞書 |\n",
    "| id_to_word | IDを単語に変換する辞書 |\n",
    "| word_matrix | 単語のベクトル行列 |\n",
    "| top    | 上位何件の類似単語を表示するかの指定 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067758832467\n",
      " i: 0.7071067758832467\n",
      " hello: 0.7071067758832467\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "#実験\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 　共起行列の改善\n",
    "\n",
    "theとcar\n",
    "\n",
    "carとdrive\n",
    "\n",
    "には関係はあるが、カウントベースでみると圧倒的に上の関係が強い関連性をもってしまう。\n",
    "\n",
    "そこで、PMI（相互情報量）というものを定義する。\n",
    "\n",
    "\n",
    "$$ PMI(x, y) = \\log_2 \\left( \\frac{P(x, y)}{P(x)P(y)} \\right) $$\n",
    "\n",
    "\n",
    "$$ PMI(x, y) = \\log_2 \\left( \\frac{P(x, y)}{P(x)P(y)} \\right) = \\log_2 \\left( \\frac{\\frac{C(x, y)}{N}}{\\frac{C(x)}{N} \\frac{C(y)}{N}} \\right) = \\log_2 \\left( \\frac{C(x, y) \\cdot N}{C(x) \\cdot C(y)} \\right) $$\n",
    "\n",
    "ここでc(x)をxが現れた回数とするとPMIは上記のようにできる。\n",
    "\n",
    "ただし、実装上は共起する回数がゼロ（つまり独立）のとき、発散してしまうので、そのときは０を返すように定義する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPMI（正の相互情報量）の実装\n",
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C) #全要素の数\n",
    "    S = np.sum(C, axis=0) #各単語の出現回数\n",
    "    total = C.shape[0] * C.shape[1] #行列の要素数\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose: #verboseがTrueの場合、進行状況を出力\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "#実験\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3) #有効桁数3桁で表示\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次元削減  \n",
    "PPMIの実装により共起行列の改善ができましたがまだ問題が残ります。  \n",
    "  \n",
    "共起行列は語彙の数だけ次元が増えていきます。  \n",
    "  \n",
    "例えばコーパスの語彙数が10万であればベクトルの次元数も10万になるのです。  \n",
    "  \n",
    "扱うのは現実的ではありません。  \n",
    "  \n",
    "さらに共起行列の中身を見るとベクトルの殆どの要素が0であることが分かります。  \n",
    "  \n",
    "これはベクトルのほとんどの要素が重要ではないことを意味し、そういったベクトルはノイズに弱く頑健性に乏しいという弱点があります。  \n",
    "  \n",
    "上の問題を解決するために**次元削減**を行います。  \n",
    "  \n",
    "ただ、ここで重要なのは単に次元数を減らすのではなく**重要な情報を残しつつ削減する**ということです。  \n",
    "  \n",
    "今回は特異値分解を用いた次元削減を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共起行列\n",
      "[0 1 0 0 0 0 0]\n",
      "PPMI\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "SVD\n",
      "[ 3.409e-01 -1.110e-16 -1.205e-01 -4.163e-16 -9.323e-01 -1.110e-16\n",
      " -2.426e-17]\n"
     ]
    }
   ],
   "source": [
    "#SVDによる次元削減\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "print(\"共起行列\")\n",
    "print(C[0]) #共起行列\n",
    "print(\"PPMI\")\n",
    "print(W[0]) #PPMI行列\n",
    "print(\"SVD\")\n",
    "print(U[0]) #SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを二次元に削減するのであれば先頭の2つの要素を取り出せばいいだけなので下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.409e-01 -1.110e-16 -1.205e-01 -4.163e-16 -9.323e-01 -1.110e-16\n",
      "  -2.426e-17]\n",
      " [ 0.000e+00 -5.976e-01  0.000e+00  1.802e-01  0.000e+00 -7.812e-01\n",
      "   0.000e+00]\n",
      " [ 4.363e-01 -5.551e-17 -5.088e-01 -2.220e-16  2.253e-01 -1.388e-17\n",
      "  -7.071e-01]\n",
      " [ 1.665e-16 -4.978e-01  2.776e-17  6.804e-01 -1.110e-16  5.378e-01\n",
      "   7.467e-17]\n",
      " [ 4.363e-01 -3.124e-17 -5.088e-01 -1.600e-16  2.253e-01 -1.302e-17\n",
      "   7.071e-01]\n",
      " [ 7.092e-01 -3.124e-17  6.839e-01 -1.600e-16  1.710e-01 -1.302e-17\n",
      "   2.314e-17]\n",
      " [-1.665e-16 -6.285e-01 -4.163e-17 -7.103e-01  2.220e-16  3.169e-01\n",
      "  -9.614e-17]]\n"
     ]
    }
   ],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.409e-01 -1.110e-16]\n"
     ]
    }
   ],
   "source": [
    "print(U[0, :2]) #2次元に圧縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#次元削減後の単語ベクトルの可視化\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "\n",
    "plt.scatter(U[:, 0], U[:, 1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで行列の改善ができたのでより大きなコーパスを使って試してみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTBデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from datasets import ptb\n",
    "\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('corpus size:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting  co-occurrence ...\n",
      "calculating PPMI ...\n",
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "100.0% done\n",
      "calculating SVD ...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from datasets import ptb\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting  co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (fast!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (slow)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " i: 0.7278802394866943\n",
      " we: 0.6299450993537903\n",
      " me: 0.5555775761604309\n",
      " do: 0.5467197299003601\n",
      " anybody: 0.533714234828949\n",
      "\n",
      "[query] year\n",
      " month: 0.6757156252861023\n",
      " quarter: 0.6243940591812134\n",
      " earlier: 0.615135133266449\n",
      " june: 0.5875838398933411\n",
      " next: 0.5865615010261536\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6211763024330139\n",
      " auto: 0.5950354337692261\n",
      " cars: 0.5102657079696655\n",
      " truck: 0.4849717319011688\n",
      " automobile: 0.4601386785507202\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7325315475463867\n",
      " motors: 0.6748571395874023\n",
      " lexus: 0.6745177507400513\n",
      " nissan: 0.645314633846283\n",
      " honda: 0.640676736831665\n"
     ]
    }
   ],
   "source": [
    "#結果\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスを使い、コンテキストの単語をカウントして、PPMI行列に変換して、SVDによる次元削減を行うことで単語ベクトルが得られた！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
