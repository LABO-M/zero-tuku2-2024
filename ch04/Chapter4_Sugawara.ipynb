{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ４章\n",
    "\n",
    "### word2vecの改良①"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#必要なライブラリのインポート\n",
    "import sys,os\n",
    "sys.path.append('/zero-tuku2-2024/common/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "３章で扱ったモデルは小さなコーパス（１文章）のみで考えました。\n",
    "しかし、通常は非常に大きなコーパスを扱うことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語彙数が100万のCBOWモデルの場合計算量が莫大で\n",
    "\n",
    ">・重み行列Wの積の計算（in,out)<br>\n",
    " ・sofutmaxレイヤでの計算<br>\n",
    "\n",
    "これらがボトルネックになってしまう。<br>また、入力層のone-hot表現は無駄な要素が多くなることも明らか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding レイヤ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力層では以下の計算がされていた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで行われているのは1つの単語に対応する行列の行を抜き出しているだけ。これを簡略化するために<br>\n",
    "\n",
    "> 「単語IDに該当する行（ベクトル）」を抜き出すためのレイヤ\n",
    "\n",
    "を作成しよう。 ここではそのレイヤをEmbeddingと呼ぶことにする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding レイヤ の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列から特定の行を抜き出すには、以下のように行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "\n",
      "[6 7 8]\n",
      "\n",
      "[15 16 17]\n"
     ]
    }
   ],
   "source": [
    "W = np.arange(21).reshape(7,3)\n",
    "print(W) \n",
    "print()\n",
    "print(W[2]) #2行目を取り出す\n",
    "print()\n",
    "print(W[5]) #5行目を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5],\n",
       "       [ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.array([1,0,3,0])\n",
    "W[idx] # 1行目、0行目、3行目、0行目を抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EmbeddingレイヤのForward()メソッドを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx # idxを抽出する行のインデックスとして保持\n",
    "        out = W[idx]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、逆伝播についても考える。\n",
    "![](picture/pict4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\vec{h} &: (1, 3) \\\\\n",
    "\\vec{W} &: (L, 3) \\text{とすると} \\\\\n",
    "\\vec{W} &= \\begin{pmatrix}\n",
    "\\vec{W_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\vec{W_L}\n",
    "\\end{pmatrix} \\text{ で, forward で流れた } \\vec{h} \\text{ について, } \\\\\n",
    "\\vec{h} &= \\vec{W_k} \\quad (1 \\leq k \\leq L) \\\\\n",
    "\\text{よって, } \\quad d\\vec{h} &= d\\vec{W_k}\n",
    "\\end{aligned}\n",
    "\n",
    "これを踏まえて実装してみる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dout):\n",
    "        dW, = self.grads # dWは重みの勾配\n",
    "        dW[...] = 0 # dWをゼロで初期化\n",
    "        dW[self.idx] = dout # 実は悪い例\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **注意**\n",
    "自分たちの目的は行列Wの更新なので、いちいちdWのような行列を作る必要はない。\n",
    "\n",
    "必要な部品は<br>\n",
    ">・更新したい行番号（idx）<br>\n",
    ">・その勾配（dout)<br>\n",
    "\n",
    "これらを保持すれば、重みWの特定の行のみを更新することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、このbackwardには問題点がある。\n",
    "![](picture/pict5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dhの各行の値をidxで指定された場所へ代入すると、dWの０番目に２つの値が代入されてしまいどちらかの値が上書きされてしまう。この問題は加算して代入することが必要。（理由は板書で説明）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正しい逆伝播の実装は次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#逆伝播の修正\n",
    "def backward(self, dout):\n",
    "    dW, = self.grads\n",
    "    dW[...] = 0\n",
    "    np.add.at(dW, self.idx, dout)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上によって入力層の重み行列の問題が解決された！<br>\n",
    "### word2vecの改良②"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残りの問題は\n",
    "> 中間層の重み行列の積の計算量<br>\n",
    ">出力層ソフトマックスの計算量<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらを解決するNegative samplingについて説明していく。\n",
    "### 多値分類から二値分類へ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結論からいうと、Negative samplingのキーになるアイデアは「**二値分類**」にあります。正確には、「多値分類」を「二値分類」で近似することです。<br>\n",
    "\n",
    "今までは、NNに対して、<br>\n",
    "「**コンテキストが『you』と『goodbye』のとき、ターゲットとなる単語はなんですか？**」と質問して学習を行っていました。<br>\n",
    "\n",
    "これを「yes」「no」で答えられる質問に置き換えます。\n",
    "\n",
    "例えば<br>\n",
    "「**コンテキストが『you』と『goodbye』のとき、ターゲットとなる単語は『say』ですか？**」という質問に答えるNNを考えることにすると、ニューロンを一つのみ用意すればよいことになります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間層と出力側の重み行列の積は、「say」に対応する列ベクトルだけを取り出し、その抽出したベクトルと中間層のニューロンの内積を取れば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力側の重みWoutには各単語IDの単語ベクトルが各列に並んで格納されています。ここでは「say」という単語ベクトルを抽出します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シグモイド関数と交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「多値分類」から「二値分類」に変えたので、スコアを出力する関数をソフトマックスからシグモイド関数へ変えます。（シグモイド関数はソフトマックスのN＝2のとき）<br>損失関数はそのまま交差エントロピーにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### シグモイド関数\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "##### 交差エントロピー誤差\n",
    "\n",
    "$$\n",
    "L = -\\left[ t \\log(y) + (1 - t) \\log(1 - y) \\right]\n",
    "$$\n",
    "\n",
    "※　tは正解ラベルで、yはシグモイド関数で計算された確率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 復習　Sigmoid with Loss レイヤ\n",
    "![](picture/pict9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、シグモイド関数の微分は以下の通り\n",
    "$$\n",
    "\\text{Sigmoid} \\quad f(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(x) = \\frac{e^x(e^x + 1) - e^x \\cdot e^x}{(e^x + 1)^2} = \\frac{e^x}{(e^x + 1)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{e^x}{e^x + 1} \\cdot \\frac{1}{e^x + 1} = f(x) \\left(1 - f(x)\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "興味があるのは、xが少し変化したときLはどれくらい変わるのかということ\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\left( -\\frac{t}{y} - \\frac{1-t}{1-y} \\right) \\cdot y(1-y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(1-t)y - (1-y)t}{y(1-y)} \\cdot y(1-y) = y - t\n",
    "$$\n",
    "\n",
    "これらの議論から二値分類を行うNNは以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力層と出力層の重み行列の計算は対応するベクトルを抜き出すことで計算量を落とせました。（Embedding）見通しをよくするため、後半部分をよりシンプルにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W_outのEmbeddingと中間層の内積をまとめたEmbedding Dotレイヤを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W) # Embeddingレイヤ\n",
    "        self.params = self.embed.params # パラメータ\n",
    "        self.grads = self.embed.grads  #v勾配\n",
    "        self.cache = None # 順伝播の入力値を保持\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.embed.params \n",
    "        dout = dout.reshape(dout.shape[0], 1) # 3つの要素のリストを(3,1)に変換し、ブロードキャストできるようにする\n",
    "\n",
    "        dtarget_W = dout * h #(3,1) * (3,7) = (3,7)\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "idx\n",
      "[0 3 1]\n",
      "target_W\n",
      "[[ 0  1  2]\n",
      " [ 9 10 11]\n",
      " [ 3  4  5]]\n",
      "h\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "target_W * h\n",
      "[[ 0  1  4]\n",
      " [27 40 55]\n",
      " [18 28 40]]\n",
      "out\n",
      "[  5 122  86]\n"
     ]
    }
   ],
   "source": [
    "#forwardの処理解説\n",
    "W = np.arange(21).reshape(7,3)\n",
    "idx = np.array([0,3,1])\n",
    "h = np.array([[0,1,2],[3,4,5],[6,7,8],]) #中間層のニューロン\n",
    "embed = Embedding(W)\n",
    "target_W = embed.forward(idx)\n",
    "out = np.sum(target_W * h, axis=1)\n",
    "#W、idx,target_W,h,target_W * h, outを表にして出力\n",
    "print(\"W\")\n",
    "print(W)\n",
    "print(\"idx\")\n",
    "print(idx)\n",
    "print(\"target_W\")\n",
    "print(target_W)\n",
    "print(\"h\")\n",
    "print(h)\n",
    "print(\"target_W * h\")\n",
    "print(target_W * h)\n",
    "print(\"out\")\n",
    "print(out) # (1,3)の行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dout\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "h\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "dout * h\n",
      "[[ 0  1  2]\n",
      " [ 6  8 10]\n",
      " [18 21 24]]\n",
      "dtarget_W\n",
      "[[ 0  1  2]\n",
      " [ 6  8 10]\n",
      " [18 21 24]]\n",
      "dh\n",
      "[[ 0  1  2]\n",
      " [18 20 22]\n",
      " [ 9 12 15]]\n"
     ]
    }
   ],
   "source": [
    "#backwardの処理解説\n",
    "dout = np.array([1,2,3])\n",
    "dout = dout.reshape(dout.shape[0], 1)\n",
    "dtarget_W = dout * h\n",
    "print(\"dout\")\n",
    "print(dout)\n",
    "print(\"h\")\n",
    "print(h)\n",
    "print(\"dout * h\")\n",
    "print(dout * h)\n",
    "print(\"dtarget_W\")\n",
    "print(dtarget_W)\n",
    "dh = dout * target_W\n",
    "print(\"dh\")\n",
    "print(dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで、正例（正しい答え）についてのみ学習を行っていました。<br>\n",
    "\n",
    "そのため、負例（誤った答え）については、どうなるか定かではありません。<br>\n",
    "\n",
    "私達は、NNが「いい分類」をしているか、「悪い分類」をしているか教えてあげる必要があります。つまり、「悪い例」を与えることで精度を効率的に上げることを考えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/pict12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、「悪い例」を教えることにしますが、これは全ての負例について行っていては、計算量を減らすという目的が達成できません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで「悪い例」を少数サンプリングして用いることにします。<br>\n",
    "\n",
    "この手法を「***Negative Sampling***」といいます。\n",
    "\n",
    "この手法では、正例と負例についてそれぞれで損失を求め、その総和を最終的な損失として学習をします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Samplingのサンプリング手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプリングはどのように行うのでしょうか。完全にランダムにサンプルするよりも、良い方法が知られています。\n",
    "\n",
    "それはコーパス内の単語の使用頻度に基づいてサンプリングする方法です。\n",
    "\n",
    "この使用頻度に応じたサンプリングにはnp.random.choice()関数を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random.choiceの使い方\n",
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wordsの中からランダムに一つ選ぶ\n",
    "words=['you','say','goodbye','I','hello','.']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['goodbye', 'say', 'hello', 'goodbye', 'hello'], dtype='<U7')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5つだけランダムに選ぶ（重複あり）\n",
    "np.random.choice(words, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['you', 'say', 'hello', 'I', 'goodbye'], dtype='<U7')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5つだけランダムに選ぶ（重複なし）\n",
    "np.random.choice(words, size=5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#確率分布に従ってサンプリング\n",
    "p=[0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "np.random.choice(words, p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random.choice()の引数は以下の３つです<br>\n",
    "size：サンプリングの回数指定<br>\n",
    "replace：重複の有無（デフォルトはTrue）<br>\n",
    "p：確率分布を指定\n",
    "\n",
    "word2vecでは与える確率分布にも一手間加えています。\n",
    "\n",
    "それは確率分布の小数乗を行うことです。\n",
    "\n",
    "この操作の意図は出現確率の低い単語の確率を少しだけ上げ、それらの単語を見捨てないようにするためです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P'(w_i) = \\frac{P(w_i)^{0.75}}{\\sum_{j=1}^{n} P(w_j)^{0.75}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64196878 0.33150408 0.02652714]\n"
     ]
    }
   ],
   "source": [
    "p=[0.7, 0.29, 0.01]\n",
    "new_p=np.power(p, 0.75) #累乗する関数np.power\n",
    "new_p /= np.sum(new_p)\n",
    "print(new_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの操作をUnigramSamplerクラスとしてまとめます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "#UnigramSamplerの実装\n",
    "import collections\n",
    "from common import config\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        #if not GPU: GPU is not defined というエラーが出るので、コメントアウトしました\n",
    "        negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        #else:\n",
    "            # GPU(cupy）で計算するときは、速度を優先\n",
    "            # 負例にターゲットが含まれるケースがある\n",
    "         #   negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "         #                                     replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0]\n",
      " [2 1]\n",
      " [3 2]]\n"
     ]
    }
   ],
   "source": [
    "#UnigramSamplerのサンプリング\n",
    "import numpy as np \n",
    "import collections\n",
    "\n",
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NegativeSamplingLossの実装\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size #負例のサンプリング数\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size) #UnigramSamplerの保持\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)] #SigmoidWithLossレイヤの保持\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)] #EmbeddingDotレイヤの保持\n",
    "\n",
    "        #lossレイヤとembed_dotレイヤは負例の数＋正例(sample_size+1)個分用意する\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, h, target): #引数は中間層のニューロンhとターゲットの単語ID\n",
    "        batch_size = target.shape[0] #バッチサイズ\n",
    "        negative_sample = self.sampler.get_negative_sample(target) #負例のサンプリング\n",
    "\n",
    "        # 正例のフォワード\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32) #正例の正解ラベルは1\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 負例のフォワード\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32) #負例の正解ラベルは0\n",
    "        for i in range(self.sample_size): #負例の数だけループ\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label) #負例の損失を加算\n",
    "        \n",
    "        #lossには正例の損失と負例の損失が合算されている\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改良版word2vecの学習\n",
    "\n",
    "CBOWモデルの実装\n",
    "\n",
    "この章ではEmbeddingレイヤとNegative sampling loss レイヤを紹介してきました。\n",
    "\n",
    "これらの改良点を踏まえて前章のSimpleCBOWクラスを改良します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改良版CBOWモデルの実装\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Embedding\n",
    "#from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embeddingレイヤを使用\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
